# OSCI Publications Migration Pipeline
#
# Unpacks OSCI publications into an intermediate database suitable for migration.
# 
# - Take a list of epub URLs
# - Grab each epub manifest URL
#   - Convert XML to JSON
#   - Unpack each manifest item into its own record
#   - Fetch each item URL 
#   - Attempt to parse them (if they're HTML-alike)
#   - Capture a wee copy of them (if they're not -- ie, type=image/* )
# - Persist them (in a db, to a REST API, etc)  

http:
  enabled: false

logger:
  level: "${LOG_LEVEL:INFO}"
  format: logfmt

# - Read the input YAML for the pubs list 
# - For each publication, unpack the package contents and annotate with `_url` for later fetching
input:
  file:
    paths: 
      - config/publications.yaml
    scanner:
      to_the_end: {}
  processors:
    - mapping: |
        root = content().parse_yaml().publications.map_each( p -> { p.id : p } ).squash()
    - unarchive:
        format: json_map
    - mapping: |
        meta pkg_name = metadata("archive_key")
        meta package = json("package_url")
        meta reader_url = json("reader_url")
    - http:
        url: '${! metadata("package") }'
        verb: GET
        headers:
          # TODO: Use the OPF mime type? */*?
          Accept: text/html; charset=utf-8
          Content-Type: text/html; charset=utf-8
    - mapping: |
        root = content().string()
    - xml:
        operator: to_json
    - mapping: |
        import "./config/osci-migration.blobl"

        let package = this.package.assign( {"_href": metadata("package") } )
        let manifest_items = this.package.manifest.item.map_each( it -> it.apply("osci_manifest_item_align") )
        root = [ $package.apply("osci_package_align") ].concat( $manifest_items.map_each( m -> m.assign({"_id": "%s/%s".format(metadata("pkg_name"),m._id) } )  ) )
    - unarchive:
        format: json_array

# Now we've got everything unpacked, split the batch into a useful size, fetch URLs and parse the results
pipeline:
  processors:
    - split: 
        size: 200
    - label: fetch_parts
      # TODO: Break up this complex branch with into `resources`

      # - Fetches URLs if they're not in cache
      # - Caches result
      # - Stores as _body_base64 (if image), _body_text (if not image)
      # - Attempts to parse as HTML, copies _body_text to _body_html if so, stores error on _meta._body_parse_error if not
      branch:
        request_map: |
          meta ident = this._id
          meta format = this._format
          meta url = this._url 
          root = if this._url != null { this._url } else { deleted() }
        processors:  
          # TODO: Handle type == image/* differently maybe
          - resource: url_cache_get
          - catch:
            - mapping: meta do_fetch = true
          - switch:
              - check: metadata("do_fetch") == true
                processors:
                  - log:
                      level: DEBUG
                      message: 'Fetching ${! metadata("url") }'
                  - http:
                      url: '${! metadata("url")  }'
                      verb: GET
                      rate_limit: easy
                      # dump_request_log_level: INFO # TODO: enable this base on an env flag (warning: very verbose!)
                      headers:
                        User-Agent: 'Mozilla/5.0 (Darwin)'
                        Accept: "*/*" # TODO: Consider using media-type? Might be too strict in migration and.. is there conneg anyway?
                  - catch:
                      - log: 
                          level: WARN
                          message: 'Could not process ${! metadata("url").string() }: ${! error() }'
                      - mapping: 'root = { "_id": metadata("url"), "_meta": {"fetch_failed": true } }'
                  - resource: url_cache_set
          - switch: 
              - check: |
                  metadata("format").has_prefix("image/")
                processors:
                  - mapping: root._body_base64 = content()
              - processors:
                  - mapping: root._body_text = content().string()
                  - branch:
                      request_map: |
                        meta body_text = this._body_text

                        root = this._body_text
                      processors:
                        - xml:
                            operator: to_json
                        # NB!! We're using xml->json only to catch the HTML parse error! ðŸ˜¿
                        - catch:
                            - mapping: meta parse_error = error()
                            # Attempt to recover from HTML parse errors by fetching (manually) cured HTML from data/errata.sqlite3
                            - sql_select:
                                driver: sqlite
                                dsn: file:data/errata.sqlite3?mode=ro
                                table: 'source_html'
                                columns: [ 'body' ]
                                where: id = ?
                                args_mapping: '[ metadata("ident") ]'
                                init_statement: 'PRAGMA journal_mode=WAL'
                            - switch:
                                # Success replace body text metadata with the cured body
                                - check: this.length() > 0
                                  processors:
                                    - mapping: |
                                        meta body_text = this.index(0).body 
                                        root._parse_error = deleted()
                                # Failure, set the body error metadata
                                - processors:
                                    - mapping: 'root._parse_error = metadata("parse_error")'
                                    - log:
                                        level: WARN
                                        message: 'Parsing HTML for ${! metadata("url") } in ${! metadata("pkg_name") } failed! Check _body_parse_error for this document'
                      result_map: |
                        root._meta._body_parse_error = this._parse_error | deleted()
                        root._body_html = if !this.exists("_parse_error") { metadata("body_text") } else { deleted() }
        result_map: |
          root.assign(this)
    # Now that it's HTML parsed, parse by format 
    - switch:
        - check: this._format == "application/xhtml+xml"
          processors:
            - label: "parse_osci_html"
              branch:
                request_map: |
                  root = this._body_html | deleted()
                processors:
                  - command:
                      name: 'node'
                      args_mapping: '[ "./alignments/parse-osci.js" ]'
                  - switch:
                      - check: 'metadata("command_stderr") != null'
                        processors:
                          - log:
                              message: '${! metadata("command_stderr") }'
                              level: WARN
                result_map: |
                  root.assign(json())
        - check: this._format == "application/osci-tk-iip-figure"
          processors:
            - label: "parse_figure_layers"
              branch:
                request_map: |
                  meta url = this._url
                  root = this._body_html | deleted()
                processors:
                  - xml:
                      operator: to_json
                  - mapping: |
                      let div_data = this.html.body.div | error()
                      let li_data = $div_data.ul.li | error()

                      let parsed_url = metadata("url").parse_url()

                      root._type = "figure"
                      root._asset_url = [ $parsed_url.scheme, "//", $parsed_url.host, "/", $parsed_url.path ].join("")

                      root._aspect = $div_data."-data-aspect"
                      root._height = $li_data."-data-height".number(0)
                      root._width = $li_data."-data-width".number(0)

                      root._layer_id = $li_data."-data-layer_id"
                      root._image_ident = $li_data."-data-ptiff_path"
                      root._image_url_stem = $li_data."-data-ptiff_sever"
                      root._thumbnail = $li_data."-data-thumb"

                      root._title = $li_data."-data-title"

                result_map: |
                  root.assign(json())

output:
  switch:
    cases:
      - check: errored()
        output:
          reject: "Record failed: ${! error() } . Message was ${! content().string() } "
      # FIXME: This logic is off for "logging" types -- the logged msg isn't sent to the persistent output!
      # TODO: For _blob assets consider a hash so we know something on the other side or in-memory thumbnail of some kind (a quickie convolution kernel on base64 data?)
      - check: this._type == "${LOG_TYPE:}"
        output:
          stdout: {}
      - check: this._format.has_prefix("image/")
        output:
          drop: {}
      - output:
          fallback:
            - sql_insert:
                driver: "sqlite"
                dsn: 'file:output/migration.sqlite3?mode=rwc'
                table: documents
                columns:
                  - id
                  - package
                  - type
                  - format
                  - title
                  - error
                  - data
                # NB: if anything needs to serialize raw HTML use `replace_all_many` see: https://github.com/benthosdev/benthos/issues/1253
                args_mapping: root = [ this._id, metadata("pkg_name"), this._type, this._format, this._title, this._meta._body_parse_error, this.string() ]
                max_in_flight: 1 # WAL should be N=2 but writes take time I guess?
                init_files:
                  - config/prepare_connex.sql
                  - config/migration_schema.sql
            - stdout: {} # FIXME: log the error instead

processor_resources:
  - label: url_cache_get
    cache:
      resource: url_cache
      operator: get
      key: '${! metadata("url") }'

  - label: url_cache_set
    cache:
      resource: url_cache
      operator: set
      key: '${! metadata("url") }'
      value: "${! content() }"

cache_resources:
  # Here only for smoke testing!
  # - label: node_cache
  #   file:
  #     directory: cache
  - label: node_cache # For nodes we create / join during processing
    memory:
      default_ttl: 5m
      init_values: {}
  - label: url_cache # URLs we request so we don't hammer endpoints
    sql:
      driver: "sqlite"
      dsn: "file:data/osci_url_cache.sqlite3?mode=rwc"
      table: documents
      key_column: id
      value_column: data
      conn_max_open: 2 # Reduce to 1 if there's contention (eg, "SQL_LOCKED" errs)
      init_statement: |
        PRAGMA journal_mode=WAL;
        CREATE TABLE IF NOT EXISTS documents (id TEXT UNIQUE ON CONFLICT REPLACE, data JSON);

# NB: ~75rps seems to be the sweet spot for publications.* -- only ~4-5 fully timed out HTTP requests (lower doesn't make it 0 ðŸ« ) 
rate_limit_resources:
  - label: easy
    local:
      count: 75 
      interval: 1s

tests: []
